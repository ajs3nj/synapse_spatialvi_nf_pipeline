"""
Orca recipe for spatialvi meta-workflow.

Orchestrates four steps:
  1. nf-synapse SYNSTAGE  : Download files from Synapse to S3
  2. make_tarball         : Create FASTQ tarballs + spatialvi samplesheet
  3. spatialvi            : Run spatialvi analysis
  4. nf-synapse SYNINDEX  : Index results back to Synapse

Usage:
  python spatialvi_workflow.py

Prerequisites:
  - pip install py-orca
  - AWS credentials configured
  - SYNAPSE_AUTH_TOKEN in Tower workspace secrets
"""

import asyncio
from dataclasses import dataclass
from pathlib import Path

from orca.services.nextflowtower import NextflowTowerOps
from orca.services.nextflowtower.models import LaunchInfo


@dataclass
class SpatialviDataset:
    """Configuration for a spatialvi workflow run."""
    
    id: str
    """Unique identifier for this dataset/run."""
    
    synstage_input_samplesheet: str
    """S3 path to the synstage input samplesheet (with syn:// URIs)."""
    
    synapse_output_folder: str
    """Synapse folder ID where results will be indexed."""
    
    bucket_name: str
    """S3 bucket for staging and outputs."""
    
    project_prefix: str
    """Prefix path within the bucket for this project."""
    
    spaceranger_reference: str
    """S3 path to spaceranger reference tarball."""
    
    spaceranger_probeset: str | None = None
    """S3 path to spaceranger probeset (optional)."""
    
    spatialvi_pipeline: str = "sagebio-ada/spatialvi"
    """GitHub repo for spatialvi pipeline."""
    
    spatialvi_revision: str = "dev"
    """Branch/tag for spatialvi pipeline."""
    
    @property
    def staging_location(self) -> str:
        """S3 URI for synstage output."""
        return f"s3://{self.bucket_name}/{self.project_prefix}/synstage"
    
    @property
    def synstage_output_samplesheet(self) -> str:
        """S3 path to the synstage output samplesheet (with S3 paths)."""
        filename = Path(self.synstage_input_samplesheet).name
        return f"{self.staging_location}/{filename}"
    
    @property
    def tarball_outdir(self) -> str:
        """S3 URI for tarball outputs."""
        return f"s3://{self.bucket_name}/{self.project_prefix}"
    
    @property
    def spatialvi_samplesheet(self) -> str:
        """S3 path to the spatialvi samplesheet (generated by make_tarball)."""
        return f"{self.tarball_outdir}/spatialvi_samplesheet.csv"
    
    @property
    def spatialvi_outdir(self) -> str:
        """S3 URI for spatialvi results."""
        return f"s3://{self.bucket_name}/{self.project_prefix}/spatialvi_results"
    
    @property
    def synstage_run_name(self) -> str:
        return f"synstage_{self.id}"
    
    @property
    def tarball_run_name(self) -> str:
        return f"make_tarball_{self.id}"
    
    @property
    def spatialvi_run_name(self) -> str:
        return f"spatialvi_{self.id}"
    
    @property
    def synindex_run_name(self) -> str:
        return f"synindex_{self.id}"


def generate_datasets() -> list[SpatialviDataset]:
    """Generate list of datasets to process.
    
    Modify this function to add your datasets.
    """
    return [
        SpatialviDataset(
            id="ANNUBP_test",
            synstage_input_samplesheet="s3://ntap-add5-project-tower-bucket/spatialvi_project/synstage_input.csv",
            synapse_output_folder="syn73722889",
            bucket_name="ntap-add5-project-tower-bucket",
            project_prefix="spatialvi_project",
            spaceranger_reference="s3://ntap-add5-project-tower-bucket/spatialvi_testing/refdata-gex-GRCh38-2020-A.tar.gz",
            spaceranger_probeset="s3://ntap-add5-project-tower-bucket/spatialvi_testing/Visium_Human_Transcriptome_Probe_Set_v2.0_GRCh38-2020-A.csv",
        )
    ]


def prepare_synstage_info(dataset: SpatialviDataset) -> LaunchInfo:
    """Generate LaunchInfo for nf-synapse SYNSTAGE."""
    return LaunchInfo(
        run_name=dataset.synstage_run_name,
        pipeline="Sage-Bionetworks-Workflows/nf-synapse",
        revision="main",
        profiles=["docker"],
        params={
            "entry": "synstage",
            "input": dataset.synstage_input_samplesheet,
            "outdir": dataset.staging_location,
        },
        workspace_secrets=["SYNAPSE_AUTH_TOKEN"],
    )


def prepare_tarball_info(dataset: SpatialviDataset) -> LaunchInfo:
    """Generate LaunchInfo for make_tarball workflow."""
    return LaunchInfo(
        run_name=dataset.tarball_run_name,
        pipeline="ajs3nj/synapse_spatialvi_nf_pipeline",
        revision="orca-orchestration",
        profiles=["docker"],
        params={
            "entry": "make_tarball",
            "input": dataset.synstage_output_samplesheet,
            "outdir": dataset.tarball_outdir,
        },
    )


def prepare_spatialvi_info(dataset: SpatialviDataset) -> LaunchInfo:
    """Generate LaunchInfo for spatialvi workflow."""
    params = {
        "input": dataset.spatialvi_samplesheet,
        "outdir": dataset.spatialvi_outdir,
        "spaceranger_reference": dataset.spaceranger_reference,
    }
    
    if dataset.spaceranger_probeset:
        params["spaceranger_probeset"] = dataset.spaceranger_probeset
    
    return LaunchInfo(
        run_name=dataset.spatialvi_run_name,
        pipeline=dataset.spatialvi_pipeline,
        revision=dataset.spatialvi_revision,
        profiles=["docker"],
        params=params,
    )


def prepare_synindex_info(dataset: SpatialviDataset) -> LaunchInfo:
    """Generate LaunchInfo for nf-synapse SYNINDEX."""
    return LaunchInfo(
        run_name=dataset.synindex_run_name,
        pipeline="Sage-Bionetworks-Workflows/nf-synapse",
        revision="main",
        profiles=["docker"],
        params={
            "entry": "synindex",
            "s3_prefix": dataset.spatialvi_outdir,
            "parent_id": dataset.synapse_output_folder,
        },
        workspace_secrets=["SYNAPSE_AUTH_TOKEN"],
    )


async def run_spatialvi_workflow(ops: NextflowTowerOps, dataset: SpatialviDataset):
    """Run the complete spatialvi meta-workflow for a single dataset."""
    
    print(f"\n{'='*60}")
    print(f"Starting spatialvi workflow for: {dataset.id}")
    print(f"{'='*60}\n")
    
    # Step 1: SYNSTAGE - Download files from Synapse
    print(f"[Step 1/4] Launching SYNSTAGE for {dataset.id}...")
    synstage_info = prepare_synstage_info(dataset)
    synstage_run_id = ops.launch_workflow(synstage_info, "spot")
    print(f"  Run ID: {synstage_run_id}")
    status = await ops.monitor_workflow(run_id=synstage_run_id, wait_time=60 * 2)
    print(f"  Status: {status}")
    if status != "SUCCEEDED":
        raise RuntimeError(f"SYNSTAGE failed for {dataset.id}: {status}")
    
    # Step 2: MAKE_TARBALL - Create tarballs and spatialvi samplesheet
    print(f"\n[Step 2/4] Launching MAKE_TARBALL for {dataset.id}...")
    tarball_info = prepare_tarball_info(dataset)
    tarball_run_id = ops.launch_workflow(tarball_info, "spot")
    print(f"  Run ID: {tarball_run_id}")
    status = await ops.monitor_workflow(run_id=tarball_run_id, wait_time=60 * 2)
    print(f"  Status: {status}")
    if status != "SUCCEEDED":
        raise RuntimeError(f"MAKE_TARBALL failed for {dataset.id}: {status}")
    
    # Step 3: SPATIALVI - Run spatialvi analysis
    print(f"\n[Step 3/4] Launching SPATIALVI for {dataset.id}...")
    spatialvi_info = prepare_spatialvi_info(dataset)
    spatialvi_run_id = ops.launch_workflow(spatialvi_info, "spot")
    print(f"  Run ID: {spatialvi_run_id}")
    status = await ops.monitor_workflow(run_id=spatialvi_run_id, wait_time=60 * 2)
    print(f"  Status: {status}")
    if status != "SUCCEEDED":
        raise RuntimeError(f"SPATIALVI failed for {dataset.id}: {status}")
    
    # Step 4: SYNINDEX - Index results back to Synapse
    print(f"\n[Step 4/4] Launching SYNINDEX for {dataset.id}...")
    synindex_info = prepare_synindex_info(dataset)
    synindex_run_id = ops.launch_workflow(synindex_info, "spot")
    print(f"  Run ID: {synindex_run_id}")
    status = await ops.monitor_workflow(run_id=synindex_run_id, wait_time=60 * 2)
    print(f"  Status: {status}")
    if status != "SUCCEEDED":
        raise RuntimeError(f"SYNINDEX failed for {dataset.id}: {status}")
    
    print(f"\n{'='*60}")
    print(f"Completed spatialvi workflow for: {dataset.id}")
    print(f"Results indexed to Synapse folder: {dataset.synapse_output_folder}")
    print(f"{'='*60}\n")
    
    return status


async def main():
    """Main entry point - run all datasets."""
    ops = NextflowTowerOps()
    datasets = generate_datasets()
    
    print(f"Starting spatialvi meta-workflow for {len(datasets)} dataset(s)")
    
    # Run all datasets (can be parallelized with asyncio.gather if needed)
    for dataset in datasets:
        await run_spatialvi_workflow(ops, dataset)
    
    print("\nAll workflows completed successfully!")


if __name__ == "__main__":
    asyncio.run(main())
