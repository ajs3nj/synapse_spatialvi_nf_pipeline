"""
Orca recipe for spatialvi meta-workflow.

Orchestrates four steps:
  1. nf-synapse SYNSTAGE  : Download files from Synapse to S3
  2. make_tarball         : Create FASTQ tarballs + spatialvi samplesheet
  3. spatialvi            : Run spatialvi analysis
  4. nf-synapse SYNINDEX  : Index results back to Synapse (per-sample or batch)

Usage:
  python spatialvi_workflow.py

Prerequisites:
  - pip install py-orca boto3
  - AWS credentials configured
  - SYNAPSE_AUTH_TOKEN in Tower workspace secrets
"""

import asyncio
import csv
import io
from dataclasses import dataclass, field
from pathlib import Path

import boto3

from orca.services.nextflowtower import NextflowTowerOps
from orca.services.nextflowtower.models import LaunchInfo


@dataclass
class SpatialviDataset:
    """Configuration for a spatialvi workflow run."""
    
    id: str
    """Unique identifier for this dataset/run."""
    
    synstage_input_samplesheet: str
    """S3 path to the synstage input samplesheet (with syn:// URIs)."""
    
    bucket_name: str
    """S3 bucket for staging and outputs."""
    
    project_prefix: str
    """Prefix path within the bucket for this project."""
    
    spaceranger_reference: str
    """S3 path to spaceranger reference tarball."""
    
    spaceranger_probeset: str | None = None
    """S3 path to spaceranger probeset (optional)."""
    
    default_synapse_output_folder: str | None = None
    """Default Synapse folder ID if not specified per-sample in samplesheet."""
    
    spatialvi_pipeline: str = "sagebio-ada/spatialvi"
    """GitHub repo for spatialvi pipeline."""
    
    spatialvi_revision: str = "dev"
    """Branch/tag for spatialvi pipeline."""
    
    @property
    def staging_location(self) -> str:
        """S3 URI for synstage output."""
        return f"s3://{self.bucket_name}/{self.project_prefix}/synstage"
    
    @property
    def synstage_output_samplesheet(self) -> str:
        """S3 path to the synstage output samplesheet (with S3 paths)."""
        filename = Path(self.synstage_input_samplesheet).name
        return f"{self.staging_location}/{filename}"
    
    @property
    def tarball_outdir(self) -> str:
        """S3 URI for tarball outputs."""
        return f"s3://{self.bucket_name}/{self.project_prefix}"
    
    @property
    def spatialvi_samplesheet(self) -> str:
        """S3 path to the spatialvi samplesheet (generated by make_tarball)."""
        return f"{self.tarball_outdir}/spatialvi_samplesheet.csv"
    
    @property
    def spatialvi_outdir(self) -> str:
        """S3 URI for spatialvi results."""
        return f"s3://{self.bucket_name}/{self.project_prefix}/spatialvi_results"
    
    @property
    def synstage_run_name(self) -> str:
        return f"synstage_{self.id}"
    
    @property
    def tarball_run_name(self) -> str:
        return f"make_tarball_{self.id}"
    
    @property
    def spatialvi_run_name(self) -> str:
        return f"spatialvi_{self.id}"
    
    def synindex_run_name(self, sample: str | None = None) -> str:
        if sample:
            return f"synindex_{self.id}_{sample}"
        return f"synindex_{self.id}"


@dataclass
class SampleSynindexInfo:
    """Per-sample information for SYNINDEX."""
    sample: str
    s3_prefix: str
    parent_id: str


def generate_datasets() -> list[SpatialviDataset]:
    """Generate list of datasets to process.
    
    Modify this function to add your datasets.
    """
    return [
        SpatialviDataset(
            id="ANNUBP_test",
            synstage_input_samplesheet="s3://ntap-add5-project-tower-bucket/spatialvi_project/synstage_input.csv",
            bucket_name="ntap-add5-project-tower-bucket",
            project_prefix="spatialvi_project",
            spaceranger_reference="s3://ntap-add5-project-tower-bucket/spatialvi_testing/refdata-gex-GRCh38-2020-A.tar.gz",
            spaceranger_probeset="s3://ntap-add5-project-tower-bucket/spatialvi_testing/Visium_Human_Transcriptome_Probe_Set_v2.0_GRCh38-2020-A.csv",
            default_synapse_output_folder="syn73722889",  # Used if results_parent_id not in samplesheet
        )
    ]


def read_samplesheet_from_s3(s3_path: str) -> list[dict]:
    """Read a CSV samplesheet from S3 and return as list of dicts."""
    # Parse S3 path
    if not s3_path.startswith("s3://"):
        raise ValueError(f"Expected S3 path, got: {s3_path}")
    
    path_parts = s3_path[5:].split("/", 1)
    bucket = path_parts[0]
    key = path_parts[1] if len(path_parts) > 1 else ""
    
    s3 = boto3.client("s3")
    response = s3.get_object(Bucket=bucket, Key=key)
    content = response["Body"].read().decode("utf-8")
    
    reader = csv.DictReader(io.StringIO(content))
    return list(reader)


def get_synindex_info_from_samplesheet(
    dataset: SpatialviDataset,
) -> list[SampleSynindexInfo]:
    """Parse samplesheet and return per-sample SYNINDEX info.
    
    If all samples have the same results_parent_id (or none specified),
    returns a single entry for batch indexing. Otherwise returns one
    entry per unique sample/parent_id combination.
    """
    rows = read_samplesheet_from_s3(dataset.synstage_output_samplesheet)
    
    # Collect unique parent_ids
    sample_info = []
    for row in rows:
        sample = row.get("sample", "")
        parent_id = row.get("results_parent_id", "").strip()
        
        if not parent_id and dataset.default_synapse_output_folder:
            parent_id = dataset.default_synapse_output_folder
        
        if not parent_id:
            raise ValueError(
                f"No results_parent_id for sample {sample} and no default specified"
            )
        
        sample_info.append(SampleSynindexInfo(
            sample=sample,
            s3_prefix=f"{dataset.spatialvi_outdir}/{sample}",
            parent_id=parent_id,
        ))
    
    # Check if all samples go to the same parent
    unique_parents = set(s.parent_id for s in sample_info)
    
    if len(unique_parents) == 1:
        # All samples go to same folder - batch index the entire results dir
        return [SampleSynindexInfo(
            sample="all",
            s3_prefix=dataset.spatialvi_outdir,
            parent_id=list(unique_parents)[0],
        )]
    
    # Different folders - need per-sample indexing
    return sample_info


def prepare_synstage_info(dataset: SpatialviDataset) -> LaunchInfo:
    """Generate LaunchInfo for nf-synapse SYNSTAGE."""
    return LaunchInfo(
        run_name=dataset.synstage_run_name,
        pipeline="Sage-Bionetworks-Workflows/nf-synapse",
        revision="main",
        profiles=["docker"],
        params={
            "entry": "synstage",
            "input": dataset.synstage_input_samplesheet,
            "outdir": dataset.staging_location,
        },
        workspace_secrets=["SYNAPSE_AUTH_TOKEN"],
    )


def prepare_tarball_info(dataset: SpatialviDataset) -> LaunchInfo:
    """Generate LaunchInfo for make_tarball workflow."""
    return LaunchInfo(
        run_name=dataset.tarball_run_name,
        pipeline="ajs3nj/synapse_spatialvi_nf_pipeline",
        revision="orca-orchestration",
        profiles=["docker"],
        params={
            "entry": "make_tarball",
            "input": dataset.synstage_output_samplesheet,
            "outdir": dataset.tarball_outdir,
        },
    )


def prepare_spatialvi_info(dataset: SpatialviDataset) -> LaunchInfo:
    """Generate LaunchInfo for spatialvi workflow."""
    params = {
        "input": dataset.spatialvi_samplesheet,
        "outdir": dataset.spatialvi_outdir,
        "spaceranger_reference": dataset.spaceranger_reference,
    }
    
    if dataset.spaceranger_probeset:
        params["spaceranger_probeset"] = dataset.spaceranger_probeset
    
    return LaunchInfo(
        run_name=dataset.spatialvi_run_name,
        pipeline=dataset.spatialvi_pipeline,
        revision=dataset.spatialvi_revision,
        profiles=["docker"],
        params=params,
    )


def prepare_synindex_info(
    dataset: SpatialviDataset,
    synindex_info: SampleSynindexInfo,
) -> LaunchInfo:
    """Generate LaunchInfo for nf-synapse SYNINDEX."""
    return LaunchInfo(
        run_name=dataset.synindex_run_name(
            None if synindex_info.sample == "all" else synindex_info.sample
        ),
        pipeline="Sage-Bionetworks-Workflows/nf-synapse",
        revision="main",
        profiles=["docker"],
        params={
            "entry": "synindex",
            "s3_prefix": synindex_info.s3_prefix,
            "parent_id": synindex_info.parent_id,
        },
        workspace_secrets=["SYNAPSE_AUTH_TOKEN"],
    )


async def run_spatialvi_workflow(ops: NextflowTowerOps, dataset: SpatialviDataset):
    """Run the complete spatialvi meta-workflow for a single dataset."""
    
    print(f"\n{'='*60}")
    print(f"Starting spatialvi workflow for: {dataset.id}")
    print(f"{'='*60}\n")
    
    # Step 1: SYNSTAGE - Download files from Synapse
    print(f"[Step 1/4] Launching SYNSTAGE for {dataset.id}...")
    synstage_info = prepare_synstage_info(dataset)
    synstage_run_id = ops.launch_workflow(synstage_info, "spot")
    print(f"  Run ID: {synstage_run_id}")
    status = await ops.monitor_workflow(run_id=synstage_run_id, wait_time=60 * 2)
    print(f"  Status: {status}")
    if status != "SUCCEEDED":
        raise RuntimeError(f"SYNSTAGE failed for {dataset.id}: {status}")
    
    # Step 2: MAKE_TARBALL - Create tarballs and spatialvi samplesheet
    print(f"\n[Step 2/4] Launching MAKE_TARBALL for {dataset.id}...")
    tarball_info = prepare_tarball_info(dataset)
    tarball_run_id = ops.launch_workflow(tarball_info, "spot")
    print(f"  Run ID: {tarball_run_id}")
    status = await ops.monitor_workflow(run_id=tarball_run_id, wait_time=60 * 2)
    print(f"  Status: {status}")
    if status != "SUCCEEDED":
        raise RuntimeError(f"MAKE_TARBALL failed for {dataset.id}: {status}")
    
    # Step 3: SPATIALVI - Run spatialvi analysis
    print(f"\n[Step 3/4] Launching SPATIALVI for {dataset.id}...")
    spatialvi_info = prepare_spatialvi_info(dataset)
    spatialvi_run_id = ops.launch_workflow(spatialvi_info, "spot")
    print(f"  Run ID: {spatialvi_run_id}")
    status = await ops.monitor_workflow(run_id=spatialvi_run_id, wait_time=60 * 2)
    print(f"  Status: {status}")
    if status != "SUCCEEDED":
        raise RuntimeError(f"SPATIALVI failed for {dataset.id}: {status}")
    
    # Step 4: SYNINDEX - Index results back to Synapse
    # Read samplesheet to determine if we need per-sample or batch indexing
    print(f"\n[Step 4/4] Preparing SYNINDEX for {dataset.id}...")
    synindex_infos = get_synindex_info_from_samplesheet(dataset)
    
    if len(synindex_infos) == 1 and synindex_infos[0].sample == "all":
        print(f"  All samples go to same Synapse folder - batch indexing")
    else:
        print(f"  Per-sample indexing required for {len(synindex_infos)} samples")
    
    for synindex_info in synindex_infos:
        sample_desc = "all results" if synindex_info.sample == "all" else synindex_info.sample
        print(f"\n  Launching SYNINDEX for {sample_desc}...")
        print(f"    S3 prefix: {synindex_info.s3_prefix}")
        print(f"    Parent ID: {synindex_info.parent_id}")
        
        launch_info = prepare_synindex_info(dataset, synindex_info)
        synindex_run_id = ops.launch_workflow(launch_info, "spot")
        print(f"    Run ID: {synindex_run_id}")
        status = await ops.monitor_workflow(run_id=synindex_run_id, wait_time=60 * 2)
        print(f"    Status: {status}")
        if status != "SUCCEEDED":
            raise RuntimeError(
                f"SYNINDEX failed for {dataset.id}/{synindex_info.sample}: {status}"
            )
    
    print(f"\n{'='*60}")
    print(f"Completed spatialvi workflow for: {dataset.id}")
    print(f"{'='*60}\n")
    
    return "SUCCEEDED"


async def main():
    """Main entry point - run all datasets."""
    ops = NextflowTowerOps()
    datasets = generate_datasets()
    
    print(f"Starting spatialvi meta-workflow for {len(datasets)} dataset(s)")
    
    # Run all datasets (can be parallelized with asyncio.gather if needed)
    for dataset in datasets:
        await run_spatialvi_workflow(ops, dataset)
    
    print("\nAll workflows completed successfully!")


if __name__ == "__main__":
    asyncio.run(main())
